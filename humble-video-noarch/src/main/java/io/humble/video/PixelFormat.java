/* ----------------------------------------------------------------------------
 * This file was automatically generated by SWIG (http://www.swig.org).
 * Version 2.0.6
 *
 * Do not make changes to this file unless you know what you are doing--modify
 * the SWIG interface file instead.
 * ----------------------------------------------------------------------------- */

package io.humble.video;
import io.humble.ferry.*;
/**
 * Picture formats and various static member functions for getting<br>
 * meta data about them.
 */
public class PixelFormat extends RefCounted {
  // JNIHelper.swg: Start generated code
  // >>>>>>>>>>>>>>>>>>>>>>>>>>>
  /**
   * This method is only here to use some references and remove
   * a Eclipse compiler warning.
   */
  @SuppressWarnings("unused")
  private void noop()
  {
    Buffer.make(null, 1);
  }
   
  private volatile long swigCPtr;

  /**
   * Internal Only.
   */
  protected PixelFormat(long cPtr, boolean cMemoryOwn) {
    super(VideoJNI.PixelFormat_SWIGUpcast(cPtr), cMemoryOwn);
    swigCPtr = cPtr;
  }
  
  /**
   * Internal Only.
   */
  protected PixelFormat(long cPtr, boolean cMemoryOwn,
      java.util.concurrent.atomic.AtomicLong ref)
  {
    super(VideoJNI.PixelFormat_SWIGUpcast(cPtr),
     cMemoryOwn, ref);
    swigCPtr = cPtr;
  }
    
  /**
   * Internal Only.  Not part of public API.
   *
   * Get the raw value of the native object that obj is proxying for.
   *   
   * @param obj The java proxy object for a native object.
   * @return The raw pointer obj is proxying for.
   */
  protected static long getCPtr(PixelFormat obj) {
    if (obj == null) return 0;
    return obj.getMyCPtr();
  }

  /**
   * Internal Only.  Not part of public API.
   *
   * Get the raw value of the native object that we're proxying for.
   *   
   * @return The raw pointer we're proxying for.
   */  
  protected long getMyCPtr() {
    if (swigCPtr == 0) throw new IllegalStateException("underlying native object already deleted");
    return swigCPtr;
  }
  
  /**
   * Create a new PixelFormat object that is actually referring to the
   * exact same underlying native object.
   *
   * @return the new Java object.
   */
  @Override
  public PixelFormat copyReference() {
    if (swigCPtr == 0)
      return null;
    else
      return new PixelFormat(swigCPtr, swigCMemOwn, getJavaRefCount());
  }

  /**
   * Compares two values, returning true if the underlying objects in native code are the same object.
   *
   * That means you can have two different Java objects, but when you do a comparison, you'll find out
   * they are the EXACT same object.
   *
   * @return True if the underlying native object is the same.  False otherwise.
   */
  public boolean equals(Object obj) {
    boolean equal = false;
    if (obj instanceof PixelFormat)
      equal = (((PixelFormat)obj).swigCPtr == this.swigCPtr);
    return equal;
  }
  
  /**
   * Get a hashable value for this object.
   *
   * @return the hashable value.
   */
  public int hashCode() {
     return (int)swigCPtr;
  }
  
  // <<<<<<<<<<<<<<<<<<<<<<<<<<<
  // JNIHelper.swg: End generated code
  
/**
 * Return the pixel format corresponding to name.<br>
 * <br>
 * If there is no pixel format with name name, then looks for a<br>
 * pixel format with the name corresponding to the native endian<br>
 * format of name.<br>
 * For example in a little-endian system, first looks for "gray16",<br>
 * then for "gray16le".<br>
 * <br>
 * Finally if no pixel format has been found, returns AV_PIX_FMT_NONE.
 */
  public static PixelFormat.Type getFormat(String name) {
    return PixelFormat.Type.swigToEnum(VideoJNI.PixelFormat_getFormat(name));
  }

/**
 * Return the short name for a pixel format, NULL in case pix_fmt is<br>
 * unknown.
 */
  public static String getFormatName(PixelFormat.Type pix_fmt) {
    return VideoJNI.PixelFormat_getFormatName(pix_fmt.swigValue());
  }

/**
 * @return a pixel format descriptor for provided pixel format or NULL if<br>
 * this pixel format is unknown.
 */
  public static PixelFormatDescriptor getDescriptor(PixelFormat.Type pix_fmt) {
    long cPtr = VideoJNI.PixelFormat_getDescriptor(pix_fmt.swigValue());
    return (cPtr == 0) ? null : new PixelFormatDescriptor(cPtr, false);
  }

/**
 * Returns the total number of pixel format descriptors known to humble video.
 */
  public static int getNumInstalledFormats() {
    return VideoJNI.PixelFormat_getNumInstalledFormats();
  }

/**
 * Returns the 'i'th pixel format descriptor that is known to humble video<br>
 * <br>
 * @param i The i'th pixel format descriptor in the list of installed descriptors.
 */
  public static PixelFormatDescriptor getInstalledFormatDescriptor(int i) {
    long cPtr = VideoJNI.PixelFormat_getInstalledFormatDescriptor(i);
    return (cPtr == 0) ? null : new PixelFormatDescriptor(cPtr, false);
  }

/**
 * @return number of planes in pix_fmt, a negative ERROR if pix_fmt is not a<br>
 * valid pixel format.
 */
  public static int getNumPlanes(PixelFormat.Type pix_fmt) {
    return VideoJNI.PixelFormat_getNumPlanes(pix_fmt.swigValue());
  }

/**
 * Utility function to swap the endianness of a pixel format.<br>
 * <br>
 * pix_fmt the pixel format<br>
 * <br>
 * @return pixel format with swapped endianness if it exists,<br>
 * otherwise AV_PIX_FMT_NONE
 */
  public static PixelFormat.Type swapEndianness(PixelFormat.Type pix_fmt) {
    return PixelFormat.Type.swigToEnum(VideoJNI.PixelFormat_swapEndianness(pix_fmt.swigValue()));
  }

/**
 * Find the buffer size that would be necessary to store an image<br>
 * with the given qualities.
 */
  public static int getBufferSizeNeeded(int width, int height, PixelFormat.Type pix_fmt) {
    return VideoJNI.PixelFormat_getBufferSizeNeeded(width, height, pix_fmt.swigValue());
  }

  /**
   * Chromaticity coordinates of the source primaries.<br>
   * These values match the ones defined by ISO/IEC 23001-8_2013 ยง 7.1.
   */
  public enum ColorPrimaries {
    COL_PRI_RESERVED0(VideoJNI.PixelFormat_COL_PRI_RESERVED0_get()),
  /**
   * also ITU-R BT1361 / IEC 61966-2-4 / SMPTE RP177 Annex B 
   */
    COL_PRI_BT709(VideoJNI.PixelFormat_COL_PRI_BT709_get()),
    COL_PRI_UNSPECIFIED(VideoJNI.PixelFormat_COL_PRI_UNSPECIFIED_get()),
    COL_PRI_RESERVED(VideoJNI.PixelFormat_COL_PRI_RESERVED_get()),
  /**
   * also FCC Title 47 Code of Federal Regulations 73.682 (a)(20) 
   */
    COL_PRI_BT470M(VideoJNI.PixelFormat_COL_PRI_BT470M_get()),
  /**
   * also ITU-R BT601-6 625 / ITU-R BT1358 625 / ITU-R BT1700 625 PAL &amp; SECAM 
   */
    COL_PRI_BT470BG(VideoJNI.PixelFormat_COL_PRI_BT470BG_get()),
  /**
   * also ITU-R BT601-6 525 / ITU-R BT1358 525 / ITU-R BT1700 NTSC 
   */
    COL_PRI_SMPTE170M(VideoJNI.PixelFormat_COL_PRI_SMPTE170M_get()),
  /**
   * functionally identical to COL_PRI_SMPTE170M 
   */
    COL_PRI_SMPTE240M(VideoJNI.PixelFormat_COL_PRI_SMPTE240M_get()),
  /**
   * colour filters using Illuminant C 
   */
    COL_PRI_FILM(VideoJNI.PixelFormat_COL_PRI_FILM_get()),
  /**
   * ITU-R BT2020 
   */
    COL_PRI_BT2020(VideoJNI.PixelFormat_COL_PRI_BT2020_get()),
  /**
   * SMPTE ST 428-1 (CIE 1931 XYZ) 
   */
    COL_PRI_SMPTE428(VideoJNI.PixelFormat_COL_PRI_SMPTE428_get()),
    COL_PRI_SMPTEST428_1(VideoJNI.PixelFormat_COL_PRI_SMPTEST428_1_get()),
  /**
   * SMPTE ST 431-2 (2011) / DCI P3 
   */
    COL_PRI_SMPTE431(VideoJNI.PixelFormat_COL_PRI_SMPTE431_get()),
  /**
   * SMPTE ST 432-1 (2010) / P3 D65 / Display P3 
   */
    COL_PRI_SMPTE432(VideoJNI.PixelFormat_COL_PRI_SMPTE432_get()),
  /**
   * JEDEC P22 phosphors 
   */
    COL_PRI_JEDEC_P22(VideoJNI.PixelFormat_COL_PRI_JEDEC_P22_get()),
  ;

    public final int swigValue() {
      return swigValue;
    }

    public static ColorPrimaries swigToEnum(int swigValue) {
      ColorPrimaries[] swigValues = ColorPrimaries.class.getEnumConstants();
      if (swigValue < swigValues.length && swigValue >= 0 && swigValues[swigValue].swigValue == swigValue)
        return swigValues[swigValue];
      for (ColorPrimaries swigEnum : swigValues)
        if (swigEnum.swigValue == swigValue)
          return swigEnum;
      throw new IllegalArgumentException("No enum " + ColorPrimaries.class + " with value " + swigValue);
    }

    @SuppressWarnings("unused")
    private ColorPrimaries() {
      this.swigValue = SwigNext.next++;
    }

    @SuppressWarnings("unused")
    private ColorPrimaries(int swigValue) {
      this.swigValue = swigValue;
      SwigNext.next = swigValue+1;
    }

    @SuppressWarnings("unused")
    private ColorPrimaries(ColorPrimaries swigEnum) {
      this.swigValue = swigEnum.swigValue;
      SwigNext.next = this.swigValue+1;
    }

    private final int swigValue;

    private static class SwigNext {
      private static int next = 0;
    }
  }

  /**
   * Color Transfer Characteristic.<br>
   * These values match the ones defined by ISO/IEC 23001-8_2013 ยง 7.2.
   */
  public enum ColorTransferCharacteristic {
    COL_TRC_RESERVED0(VideoJNI.PixelFormat_COL_TRC_RESERVED0_get()),
  /**
   * also ITU-R BT1361 
   */
    COL_TRC_BT709(VideoJNI.PixelFormat_COL_TRC_BT709_get()),
    COL_TRC_UNSPECIFIED(VideoJNI.PixelFormat_COL_TRC_UNSPECIFIED_get()),
    COL_TRC_RESERVED(VideoJNI.PixelFormat_COL_TRC_RESERVED_get()),
  /**
   * also ITU-R BT470M / ITU-R BT1700 625 PAL &amp; SECAM 
   */
    COL_TRC_GAMMA22(VideoJNI.PixelFormat_COL_TRC_GAMMA22_get()),
  /**
   * also ITU-R BT470BG 
   */
    COL_TRC_GAMMA28(VideoJNI.PixelFormat_COL_TRC_GAMMA28_get()),
  /**
   * also ITU-R BT601-6 525 or 625 / ITU-R BT1358 525 or 625 / ITU-R BT1700 NTSC 
   */
    COL_TRC_SMPTE170M(VideoJNI.PixelFormat_COL_TRC_SMPTE170M_get()),
    COL_TRC_SMPTE240M(VideoJNI.PixelFormat_COL_TRC_SMPTE240M_get()),
  /**
   * "Linear transfer characteristics" 
   */
    COL_TRC_LINEAR(VideoJNI.PixelFormat_COL_TRC_LINEAR_get()),
  /**
   * "Logarithmic transfer characteristic (100:1 range)" 
   */
    COL_TRC_LOG(VideoJNI.PixelFormat_COL_TRC_LOG_get()),
  /**
   * "Logarithmic transfer characteristic (100 * Sqrt(10) : 1 range)" 
   */
    COL_TRC_LOG_SQRT(VideoJNI.PixelFormat_COL_TRC_LOG_SQRT_get()),
  /**
   * IEC 61966-2-4 
   */
    COL_TRC_IEC61966_2_4(VideoJNI.PixelFormat_COL_TRC_IEC61966_2_4_get()),
  /**
   * ITU-R BT1361 Extended Colour Gamut 
   */
    COL_TRC_BT1361_ECG(VideoJNI.PixelFormat_COL_TRC_BT1361_ECG_get()),
  /**
   * IEC 61966-2-1 (sRGB or sYCC) 
   */
    COL_TRC_IEC61966_2_1(VideoJNI.PixelFormat_COL_TRC_IEC61966_2_1_get()),
  /**
   * ITU-R BT2020 for 10-bit system 
   */
    COL_TRC_BT2020_10(VideoJNI.PixelFormat_COL_TRC_BT2020_10_get()),
  /**
   * ITU-R BT2020 for 12-bit system 
   */
    COL_TRC_BT2020_12(VideoJNI.PixelFormat_COL_TRC_BT2020_12_get()),
  /**
   * SMPTE ST 2084 for 10-, 12-, 14- and 16-bit systems 
   */
    COL_TRC_SMPTE2084(VideoJNI.PixelFormat_COL_TRC_SMPTE2084_get()),
    COL_TRC_SMPTEST2084(VideoJNI.PixelFormat_COL_TRC_SMPTEST2084_get()),
  /**
   * SMPTE ST 428-1 
   */
    COL_TRC_SMPTE428(VideoJNI.PixelFormat_COL_TRC_SMPTE428_get()),
    COL_TRC_SMPTEST428_1(VideoJNI.PixelFormat_COL_TRC_SMPTEST428_1_get()),
  /**
   * ARIB STD-B67, known as "Hybrid log-gamma" 
   */
    COL_TRC_ARIB_STD_B67(VideoJNI.PixelFormat_COL_TRC_ARIB_STD_B67_get()),
  ;

    public final int swigValue() {
      return swigValue;
    }

    public static ColorTransferCharacteristic swigToEnum(int swigValue) {
      ColorTransferCharacteristic[] swigValues = ColorTransferCharacteristic.class.getEnumConstants();
      if (swigValue < swigValues.length && swigValue >= 0 && swigValues[swigValue].swigValue == swigValue)
        return swigValues[swigValue];
      for (ColorTransferCharacteristic swigEnum : swigValues)
        if (swigEnum.swigValue == swigValue)
          return swigEnum;
      throw new IllegalArgumentException("No enum " + ColorTransferCharacteristic.class + " with value " + swigValue);
    }

    @SuppressWarnings("unused")
    private ColorTransferCharacteristic() {
      this.swigValue = SwigNext.next++;
    }

    @SuppressWarnings("unused")
    private ColorTransferCharacteristic(int swigValue) {
      this.swigValue = swigValue;
      SwigNext.next = swigValue+1;
    }

    @SuppressWarnings("unused")
    private ColorTransferCharacteristic(ColorTransferCharacteristic swigEnum) {
      this.swigValue = swigEnum.swigValue;
      SwigNext.next = this.swigValue+1;
    }

    private final int swigValue;

    private static class SwigNext {
      private static int next = 0;
    }
  }

  /**
   * YUV colorspace type.<br>
   * These values match the ones defined by ISO/IEC 23001-8_2013 ยง 7.3.
   */
  public enum ColorSpace {
  /**
   * order of coefficients is actually GBR, also IEC 61966-2-1 (sRGB) 
   */
    COL_SPC_RGB(VideoJNI.PixelFormat_COL_SPC_RGB_get()),
  /**
   * also ITU-R BT1361 / IEC 61966-2-4 xvYCC709 / SMPTE RP177 Annex B 
   */
    COL_SPC_BT709(VideoJNI.PixelFormat_COL_SPC_BT709_get()),
    COL_SPC_UNSPECIFIED(VideoJNI.PixelFormat_COL_SPC_UNSPECIFIED_get()),
    COL_SPC_RESERVED(VideoJNI.PixelFormat_COL_SPC_RESERVED_get()),
  /**
   * FCC Title 47 Code of Federal Regulations 73.682 (a)(20) 
   */
    COL_SPC_FCC(VideoJNI.PixelFormat_COL_SPC_FCC_get()),
  /**
   * also ITU-R BT601-6 625 / ITU-R BT1358 625 / ITU-R BT1700 625 PAL &amp; SECAM / IEC 61966-2-4 xvYCC601 
   */
    COL_SPC_BT470BG(VideoJNI.PixelFormat_COL_SPC_BT470BG_get()),
  /**
   * also ITU-R BT601-6 525 / ITU-R BT1358 525 / ITU-R BT1700 NTSC 
   */
    COL_SPC_SMPTE170M(VideoJNI.PixelFormat_COL_SPC_SMPTE170M_get()),
  /**
   * functionally identical to SMPTE170M 
   */
    COL_SPC_SMPTE240M(VideoJNI.PixelFormat_COL_SPC_SMPTE240M_get()),
  /**
   * Used by Dirac / VC-2 and H.264 FRext, see ITU-T SG16 
   */
    COL_SPC_YCGCO(VideoJNI.PixelFormat_COL_SPC_YCGCO_get()),
    COL_SPC_YCOCG(VideoJNI.PixelFormat_COL_SPC_YCOCG_get()),
  /**
   * ITU-R BT2020 non-constant luminance system 
   */
    COL_SPC_BT2020_NCL(VideoJNI.PixelFormat_COL_SPC_BT2020_NCL_get()),
  /**
   * ITU-R BT2020 constant luminance system 
   */
    COL_SPC_BT2020_CL(VideoJNI.PixelFormat_COL_SPC_BT2020_CL_get()),
  /**
   * SMPTE 2085, Y'D'zD'x 
   */
    COL_SPC_SMPTE2085(VideoJNI.PixelFormat_COL_SPC_SMPTE2085_get()),
  /**
   * Chromaticity-derived non-constant luminance system 
   */
    COL_SPC_CHROMA_DERIVED_NCL(VideoJNI.PixelFormat_COL_SPC_CHROMA_DERIVED_NCL_get()),
  /**
   * Chromaticity-derived constant luminance system 
   */
    COL_SPC_CHROMA_DERIVED_CL(VideoJNI.PixelFormat_COL_SPC_CHROMA_DERIVED_CL_get()),
  /**
   * ITU-R BT.2100-0, ICtCp 
   */
    COL_SPC_ICTCP(VideoJNI.PixelFormat_COL_SPC_ICTCP_get()),
  ;

    public final int swigValue() {
      return swigValue;
    }

    public static ColorSpace swigToEnum(int swigValue) {
      ColorSpace[] swigValues = ColorSpace.class.getEnumConstants();
      if (swigValue < swigValues.length && swigValue >= 0 && swigValues[swigValue].swigValue == swigValue)
        return swigValues[swigValue];
      for (ColorSpace swigEnum : swigValues)
        if (swigEnum.swigValue == swigValue)
          return swigEnum;
      throw new IllegalArgumentException("No enum " + ColorSpace.class + " with value " + swigValue);
    }

    @SuppressWarnings("unused")
    private ColorSpace() {
      this.swigValue = SwigNext.next++;
    }

    @SuppressWarnings("unused")
    private ColorSpace(int swigValue) {
      this.swigValue = swigValue;
      SwigNext.next = swigValue+1;
    }

    @SuppressWarnings("unused")
    private ColorSpace(ColorSpace swigEnum) {
      this.swigValue = swigEnum.swigValue;
      SwigNext.next = this.swigValue+1;
    }

    private final int swigValue;

    private static class SwigNext {
      private static int next = 0;
    }
  }

  /**
   * MPEG vs JPEG YUV range.
   */
  public enum ColorRange {
    COL_RANGE_UNSPECIFIED(VideoJNI.PixelFormat_COL_RANGE_UNSPECIFIED_get()),
  /**
   * the normal 219*2^(n-8) "MPEG" YUV ranges 
   */
    COL_RANGE_MPEG(VideoJNI.PixelFormat_COL_RANGE_MPEG_get()),
  /**
   * the normal     2^n-1   "JPEG" YUV ranges 
   */
    COL_RANGE_JPEG(VideoJNI.PixelFormat_COL_RANGE_JPEG_get()),
  ;

    public final int swigValue() {
      return swigValue;
    }

    public static ColorRange swigToEnum(int swigValue) {
      ColorRange[] swigValues = ColorRange.class.getEnumConstants();
      if (swigValue < swigValues.length && swigValue >= 0 && swigValues[swigValue].swigValue == swigValue)
        return swigValues[swigValue];
      for (ColorRange swigEnum : swigValues)
        if (swigEnum.swigValue == swigValue)
          return swigEnum;
      throw new IllegalArgumentException("No enum " + ColorRange.class + " with value " + swigValue);
    }

    @SuppressWarnings("unused")
    private ColorRange() {
      this.swigValue = SwigNext.next++;
    }

    @SuppressWarnings("unused")
    private ColorRange(int swigValue) {
      this.swigValue = swigValue;
      SwigNext.next = swigValue+1;
    }

    @SuppressWarnings("unused")
    private ColorRange(ColorRange swigEnum) {
      this.swigValue = swigEnum.swigValue;
      SwigNext.next = this.swigValue+1;
    }

    private final int swigValue;

    private static class SwigNext {
      private static int next = 0;
    }
  }

  /**
   * Location of chroma samples.<br>
   * <p><br>
   * Illustration showing the location of the first (top left) chroma sample of the<br>
   * image, the left shows only luma, the right<br>
   * shows the location of the chroma sample, the 2 could be imagined to overlay<br>
   * each other but are drawn separately due to limitations of ASCII<br>
   * </p><br>
   * <pre><br>
   * <br>
   *                1st 2nd       1st 2nd horizontal luma sample positions<br>
   *                 v   v         v   v<br>
   *                 ______        ______<br>
   * 1st luma line &gt; |X   X ...    |3 4 X ...     X are luma samples,<br>
   *                |             |1 2           1-6 are possible chroma positions<br>
   * 2nd luma line &gt; |X   X ...    |5 6 X ...     0 is undefined/unknown position<br>
   * </pre>
   */
  public enum ChromaLocation {
    CHROMA_LOC_UNSPECIFIED(VideoJNI.PixelFormat_CHROMA_LOC_UNSPECIFIED_get()),
  /**
   * MPEG-2/4 4:2:0, H.264 default for 4:2:0 
   */
    CHROMA_LOC_LEFT(VideoJNI.PixelFormat_CHROMA_LOC_LEFT_get()),
  /**
   * MPEG-1 4:2:0, JPEG 4:2:0, H.263 4:2:0 
   */
    CHROMA_LOC_CENTER(VideoJNI.PixelFormat_CHROMA_LOC_CENTER_get()),
  /**
   * ITU-R 601, SMPTE 274M 296M S314M(DV 4:1:1), mpeg2 4:2:2 
   */
    CHROMA_LOC_TOPLEFT(VideoJNI.PixelFormat_CHROMA_LOC_TOPLEFT_get()),
    CHROMA_LOC_TOP(VideoJNI.PixelFormat_CHROMA_LOC_TOP_get()),
    CHROMA_LOC_BOTTOMLEFT(VideoJNI.PixelFormat_CHROMA_LOC_BOTTOMLEFT_get()),
    CHROMA_LOC_BOTTOM(VideoJNI.PixelFormat_CHROMA_LOC_BOTTOM_get()),
  ;

    public final int swigValue() {
      return swigValue;
    }

    public static ChromaLocation swigToEnum(int swigValue) {
      ChromaLocation[] swigValues = ChromaLocation.class.getEnumConstants();
      if (swigValue < swigValues.length && swigValue >= 0 && swigValues[swigValue].swigValue == swigValue)
        return swigValues[swigValue];
      for (ChromaLocation swigEnum : swigValues)
        if (swigEnum.swigValue == swigValue)
          return swigEnum;
      throw new IllegalArgumentException("No enum " + ChromaLocation.class + " with value " + swigValue);
    }

    @SuppressWarnings("unused")
    private ChromaLocation() {
      this.swigValue = SwigNext.next++;
    }

    @SuppressWarnings("unused")
    private ChromaLocation(int swigValue) {
      this.swigValue = swigValue;
      SwigNext.next = swigValue+1;
    }

    @SuppressWarnings("unused")
    private ChromaLocation(ChromaLocation swigEnum) {
      this.swigValue = swigEnum.swigValue;
      SwigNext.next = this.swigValue+1;
    }

    private final int swigValue;

    private static class SwigNext {
      private static int next = 0;
    }
  }

  /**
   * The order of the fields in interlaced video. Video only.
   */
  public enum FieldOrder {
  /**
   * Unknown 
   */
    FIELD_UNKNOWN(VideoJNI.PixelFormat_FIELD_UNKNOWN_get()),
  /**
   * Progressive 
   */
    FIELD_PROGRESSIVE(VideoJNI.PixelFormat_FIELD_PROGRESSIVE_get()),
  /**
   * Top coded_first, top displayed first 
   */
    FIELD_TT(VideoJNI.PixelFormat_FIELD_TT_get()),
  /**
   * Bottom coded first, bottom displayed first 
   */
    FIELD_BB(VideoJNI.PixelFormat_FIELD_BB_get()),
  /**
   * Top coded first, bottom displayed first 
   */
    FIELD_TB(VideoJNI.PixelFormat_FIELD_TB_get()),
  /**
   * Bottom coded first, top displayed first 
   */
    FIELD_BT(VideoJNI.PixelFormat_FIELD_BT_get()),
  ;

    public final int swigValue() {
      return swigValue;
    }

    public static FieldOrder swigToEnum(int swigValue) {
      FieldOrder[] swigValues = FieldOrder.class.getEnumConstants();
      if (swigValue < swigValues.length && swigValue >= 0 && swigValues[swigValue].swigValue == swigValue)
        return swigValues[swigValue];
      for (FieldOrder swigEnum : swigValues)
        if (swigEnum.swigValue == swigValue)
          return swigEnum;
      throw new IllegalArgumentException("No enum " + FieldOrder.class + " with value " + swigValue);
    }

    @SuppressWarnings("unused")
    private FieldOrder() {
      this.swigValue = SwigNext.next++;
    }

    @SuppressWarnings("unused")
    private FieldOrder(int swigValue) {
      this.swigValue = swigValue;
      SwigNext.next = swigValue+1;
    }

    @SuppressWarnings("unused")
    private FieldOrder(FieldOrder swigEnum) {
      this.swigValue = swigEnum.swigValue;
      SwigNext.next = this.swigValue+1;
    }

    private final int swigValue;

    private static class SwigNext {
      private static int next = 0;
    }
  }

  /**
   * Pixel format.<br>
   * <p><br>
   * Note: <br>
   * PIX_FMT_RGB32 is handled in an endian-specific manner. An RGBA<br>
   * color is put together as:<br>
   * </p><br>
   * <p><br>
   *  (A &lt;< 24) | (R &lt;< 16) | (G &lt;< 8) | B<br>
   * </p><br>
   * <p><br>
   * This is stored as BGRA on little-endian CPU architectures and ARGB on<br>
   * big-endian CPUs.<br>
   * </p><br>
   * <p><br>
   * Note: <br>
   * If the resolution is not a multiple of the chroma subsampling factor<br>
   * then the chroma plane resolution must be rounded up.<br>
   * </p><br>
   * <p><br>
   * When the pixel format is palettized RGB32 (PIX_FMT_PAL8), the palettized<br>
   * image data is stored in AVFrame.data[0]. The palette is transported in<br>
   * AVFrame.data[1], is 1024 bytes long (256 4-byte entries) and is<br>
   * formatted the same as in PIX_FMT_RGB32 described above (i.e., it is<br>
   * also endian-specific). Note also that the individual RGB32 palette<br>
   * components stored in AVFrame.data[1] should be in the range 0..255.<br>
   * This is important as many custom PAL8 video codecs that were designed<br>
   * to run on the IBM VGA graphics adapter use 6-bit palette components.<br>
   * </p><br>
   * <p><br>
   * For all the 8 bits per pixel formats, an RGB32 palette is in data[1] like<br>
   * for pal8. This palette is filled in automatically by the function<br>
   * allocating the picture.<br>
   * </p>
   */
  public enum Type {
    PIX_FMT_NONE(VideoJNI.PixelFormat_PIX_FMT_NONE_get()),
  /**
   * planar YUV 4:2:0, 12bpp, (1 Cr &amp; Cb sample per 2x2 Y samples)
   */
    PIX_FMT_YUV420P(VideoJNI.PixelFormat_PIX_FMT_YUV420P_get()),
  /**
   * packed YUV 4:2:2, 16bpp, Y0 Cb Y1 Cr
   */
    PIX_FMT_YUYV422(VideoJNI.PixelFormat_PIX_FMT_YUYV422_get()),
  /**
   * packed RGB 8:8:8, 24bpp, RGBRGB...
   */
    PIX_FMT_RGB24(VideoJNI.PixelFormat_PIX_FMT_RGB24_get()),
  /**
   * packed RGB 8:8:8, 24bpp, BGRBGR...
   */
    PIX_FMT_BGR24(VideoJNI.PixelFormat_PIX_FMT_BGR24_get()),
  /**
   * planar YUV 4:2:2, 16bpp, (1 Cr &amp; Cb sample per 2x1 Y samples)
   */
    PIX_FMT_YUV422P(VideoJNI.PixelFormat_PIX_FMT_YUV422P_get()),
  /**
   * planar YUV 4:4:4, 24bpp, (1 Cr &amp; Cb sample per 1x1 Y samples)
   */
    PIX_FMT_YUV444P(VideoJNI.PixelFormat_PIX_FMT_YUV444P_get()),
  /**
   * planar YUV 4:1:0,  9bpp, (1 Cr &amp; Cb sample per 4x4 Y samples)
   */
    PIX_FMT_YUV410P(VideoJNI.PixelFormat_PIX_FMT_YUV410P_get()),
  /**
   * planar YUV 4:1:1, 12bpp, (1 Cr &amp; Cb sample per 4x1 Y samples)
   */
    PIX_FMT_YUV411P(VideoJNI.PixelFormat_PIX_FMT_YUV411P_get()),
  /**
   * Y        ,  8bpp
   */
    PIX_FMT_GRAY8(VideoJNI.PixelFormat_PIX_FMT_GRAY8_get()),
  /**
   * Y        ,  1bpp, 0 is white, 1 is black, in each byte pixels are ordered from the msb to the lsb
   */
    PIX_FMT_MONOWHITE(VideoJNI.PixelFormat_PIX_FMT_MONOWHITE_get()),
  /**
   * Y        ,  1bpp, 0 is black, 1 is white, in each byte pixels are ordered from the msb to the lsb
   */
    PIX_FMT_MONOBLACK(VideoJNI.PixelFormat_PIX_FMT_MONOBLACK_get()),
  /**
   * 8 bits with AV_PIX_FMT_RGB32 palette
   */
    PIX_FMT_PAL8(VideoJNI.PixelFormat_PIX_FMT_PAL8_get()),
  /**
   * planar YUV 4:2:0, 12bpp, full scale (JPEG), deprecated in favor of AV_PIX_FMT_YUV420P and setting color_range
   */
    PIX_FMT_YUVJ420P(VideoJNI.PixelFormat_PIX_FMT_YUVJ420P_get()),
  /**
   * planar YUV 4:2:2, 16bpp, full scale (JPEG), deprecated in favor of AV_PIX_FMT_YUV422P and setting color_range
   */
    PIX_FMT_YUVJ422P(VideoJNI.PixelFormat_PIX_FMT_YUVJ422P_get()),
  /**
   * planar YUV 4:4:4, 24bpp, full scale (JPEG), deprecated in favor of AV_PIX_FMT_YUV444P and setting color_range
   */
    PIX_FMT_YUVJ444P(VideoJNI.PixelFormat_PIX_FMT_YUVJ444P_get()),
  /**
   * packed YUV 4:2:2, 16bpp, Cb Y0 Cr Y1
   */
    PIX_FMT_UYVY422(VideoJNI.PixelFormat_PIX_FMT_UYVY422_get()),
  /**
   * packed YUV 4:1:1, 12bpp, Cb Y0 Y1 Cr Y2 Y3
   */
    PIX_FMT_UYYVYY411(VideoJNI.PixelFormat_PIX_FMT_UYYVYY411_get()),
  /**
   * packed RGB 3:3:2,  8bpp, (msb)2B 3G 3R(lsb)
   */
    PIX_FMT_BGR8(VideoJNI.PixelFormat_PIX_FMT_BGR8_get()),
  /**
   * packed RGB 1:2:1 bitstream,  4bpp, (msb)1B 2G 1R(lsb), a byte contains two pixels, the first pixel in the byte is the one composed by the 4 msb bits
   */
    PIX_FMT_BGR4(VideoJNI.PixelFormat_PIX_FMT_BGR4_get()),
  /**
   * packed RGB 1:2:1,  8bpp, (msb)1B 2G 1R(lsb)
   */
    PIX_FMT_BGR4_BYTE(VideoJNI.PixelFormat_PIX_FMT_BGR4_BYTE_get()),
  /**
   * packed RGB 3:3:2,  8bpp, (msb)2R 3G 3B(lsb)
   */
    PIX_FMT_RGB8(VideoJNI.PixelFormat_PIX_FMT_RGB8_get()),
  /**
   * packed RGB 1:2:1 bitstream,  4bpp, (msb)1R 2G 1B(lsb), a byte contains two pixels, the first pixel in the byte is the one composed by the 4 msb bits
   */
    PIX_FMT_RGB4(VideoJNI.PixelFormat_PIX_FMT_RGB4_get()),
  /**
   * packed RGB 1:2:1,  8bpp, (msb)1R 2G 1B(lsb)
   */
    PIX_FMT_RGB4_BYTE(VideoJNI.PixelFormat_PIX_FMT_RGB4_BYTE_get()),
  /**
   * planar YUV 4:2:0, 12bpp, 1 plane for Y and 1 plane for the UV components, which are interleaved (first byte U and the following byte V)
   */
    PIX_FMT_NV12(VideoJNI.PixelFormat_PIX_FMT_NV12_get()),
  /**
   * as above, but U and V bytes are swapped
   */
    PIX_FMT_NV21(VideoJNI.PixelFormat_PIX_FMT_NV21_get()),
  /**
   * packed ARGB 8:8:8:8, 32bpp, ARGBARGB...
   */
    PIX_FMT_ARGB(VideoJNI.PixelFormat_PIX_FMT_ARGB_get()),
  /**
   * packed RGBA 8:8:8:8, 32bpp, RGBARGBA...
   */
    PIX_FMT_RGBA(VideoJNI.PixelFormat_PIX_FMT_RGBA_get()),
  /**
   * packed ABGR 8:8:8:8, 32bpp, ABGRABGR...
   */
    PIX_FMT_ABGR(VideoJNI.PixelFormat_PIX_FMT_ABGR_get()),
  /**
   * packed BGRA 8:8:8:8, 32bpp, BGRABGRA...
   */
    PIX_FMT_BGRA(VideoJNI.PixelFormat_PIX_FMT_BGRA_get()),
  /**
   * Y        , 16bpp, big-endian
   */
    PIX_FMT_GRAY16BE(VideoJNI.PixelFormat_PIX_FMT_GRAY16BE_get()),
  /**
   * Y        , 16bpp, little-endian
   */
    PIX_FMT_GRAY16LE(VideoJNI.PixelFormat_PIX_FMT_GRAY16LE_get()),
  /**
   * planar YUV 4:4:0 (1 Cr &amp; Cb sample per 1x2 Y samples)
   */
    PIX_FMT_YUV440P(VideoJNI.PixelFormat_PIX_FMT_YUV440P_get()),
  /**
   * planar YUV 4:4:0 full scale (JPEG), deprecated in favor of AV_PIX_FMT_YUV440P and setting color_range
   */
    PIX_FMT_YUVJ440P(VideoJNI.PixelFormat_PIX_FMT_YUVJ440P_get()),
  /**
   * planar YUV 4:2:0, 20bpp, (1 Cr &amp; Cb sample per 2x2 Y &amp; A samples)
   */
    PIX_FMT_YUVA420P(VideoJNI.PixelFormat_PIX_FMT_YUVA420P_get()),
  /**
   * packed RGB 16:16:16, 48bpp, 16R, 16G, 16B, the 2-byte value for each R/G/B component is stored as big-endian
   */
    PIX_FMT_RGB48BE(VideoJNI.PixelFormat_PIX_FMT_RGB48BE_get()),
  /**
   * packed RGB 16:16:16, 48bpp, 16R, 16G, 16B, the 2-byte value for each R/G/B component is stored as little-endian
   */
    PIX_FMT_RGB48LE(VideoJNI.PixelFormat_PIX_FMT_RGB48LE_get()),
  /**
   * packed RGB 5:6:5, 16bpp, (msb)   5R 6G 5B(lsb), big-endian
   */
    PIX_FMT_RGB565BE(VideoJNI.PixelFormat_PIX_FMT_RGB565BE_get()),
  /**
   * packed RGB 5:6:5, 16bpp, (msb)   5R 6G 5B(lsb), little-endian
   */
    PIX_FMT_RGB565LE(VideoJNI.PixelFormat_PIX_FMT_RGB565LE_get()),
  /**
   * packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), big-endian   , X=unused/undefined
   */
    PIX_FMT_RGB555BE(VideoJNI.PixelFormat_PIX_FMT_RGB555BE_get()),
  /**
   * packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), little-endian, X=unused/undefined
   */
    PIX_FMT_RGB555LE(VideoJNI.PixelFormat_PIX_FMT_RGB555LE_get()),
  /**
   * packed BGR 5:6:5, 16bpp, (msb)   5B 6G 5R(lsb), big-endian
   */
    PIX_FMT_BGR565BE(VideoJNI.PixelFormat_PIX_FMT_BGR565BE_get()),
  /**
   * packed BGR 5:6:5, 16bpp, (msb)   5B 6G 5R(lsb), little-endian
   */
    PIX_FMT_BGR565LE(VideoJNI.PixelFormat_PIX_FMT_BGR565LE_get()),
  /**
   * packed BGR 5:5:5, 16bpp, (msb)1X 5B 5G 5R(lsb), big-endian   , X=unused/undefined
   */
    PIX_FMT_BGR555BE(VideoJNI.PixelFormat_PIX_FMT_BGR555BE_get()),
  /**
   * packed BGR 5:5:5, 16bpp, (msb)1X 5B 5G 5R(lsb), little-endian, X=unused/undefined
   */
    PIX_FMT_BGR555LE(VideoJNI.PixelFormat_PIX_FMT_BGR555LE_get()),
  /**
   *  Hardware acceleration through VA-API, data[3] contains a<br>
   *  VASurfaceID.
   */
    PIX_FMT_VAAPI(VideoJNI.PixelFormat_PIX_FMT_VAAPI_get()),
  /**
   * planar YUV 4:2:0, 24bpp, (1 Cr &amp; Cb sample per 2x2 Y samples), little-endian
   */
    PIX_FMT_YUV420P16LE(VideoJNI.PixelFormat_PIX_FMT_YUV420P16LE_get()),
  /**
   * planar YUV 4:2:0, 24bpp, (1 Cr &amp; Cb sample per 2x2 Y samples), big-endian
   */
    PIX_FMT_YUV420P16BE(VideoJNI.PixelFormat_PIX_FMT_YUV420P16BE_get()),
  /**
   * planar YUV 4:2:2, 32bpp, (1 Cr &amp; Cb sample per 2x1 Y samples), little-endian
   */
    PIX_FMT_YUV422P16LE(VideoJNI.PixelFormat_PIX_FMT_YUV422P16LE_get()),
  /**
   * planar YUV 4:2:2, 32bpp, (1 Cr &amp; Cb sample per 2x1 Y samples), big-endian
   */
    PIX_FMT_YUV422P16BE(VideoJNI.PixelFormat_PIX_FMT_YUV422P16BE_get()),
  /**
   * planar YUV 4:4:4, 48bpp, (1 Cr &amp; Cb sample per 1x1 Y samples), little-endian
   */
    PIX_FMT_YUV444P16LE(VideoJNI.PixelFormat_PIX_FMT_YUV444P16LE_get()),
  /**
   * planar YUV 4:4:4, 48bpp, (1 Cr &amp; Cb sample per 1x1 Y samples), big-endian
   */
    PIX_FMT_YUV444P16BE(VideoJNI.PixelFormat_PIX_FMT_YUV444P16BE_get()),
  /**
   * HW decoding through DXVA2, Picture.data[3] contains a LPDIRECT3DSURFACE9 pointer
   */
    PIX_FMT_DXVA2_VLD(VideoJNI.PixelFormat_PIX_FMT_DXVA2_VLD_get()),
  /**
   * packed RGB 4:4:4, 16bpp, (msb)4X 4R 4G 4B(lsb), little-endian, X=unused/undefined
   */
    PIX_FMT_RGB444LE(VideoJNI.PixelFormat_PIX_FMT_RGB444LE_get()),
  /**
   * packed RGB 4:4:4, 16bpp, (msb)4X 4R 4G 4B(lsb), big-endian,    X=unused/undefined
   */
    PIX_FMT_RGB444BE(VideoJNI.PixelFormat_PIX_FMT_RGB444BE_get()),
  /**
   * packed BGR 4:4:4, 16bpp, (msb)4X 4B 4G 4R(lsb), little-endian, X=unused/undefined
   */
    PIX_FMT_BGR444LE(VideoJNI.PixelFormat_PIX_FMT_BGR444LE_get()),
  /**
   * packed BGR 4:4:4, 16bpp, (msb)4X 4B 4G 4R(lsb), big-endian,    X=unused/undefined
   */
    PIX_FMT_BGR444BE(VideoJNI.PixelFormat_PIX_FMT_BGR444BE_get()),
  /**
   * 8 bits gray, 8 bits alpha
   */
    PIX_FMT_YA8(VideoJNI.PixelFormat_PIX_FMT_YA8_get()),
  /**
   * alias for AV_PIX_FMT_YA8
   */
    PIX_FMT_Y400A(VideoJNI.PixelFormat_PIX_FMT_Y400A_get()),
  /**
   * alias for AV_PIX_FMT_YA8
   */
    PIX_FMT_GRAY8A(VideoJNI.PixelFormat_PIX_FMT_GRAY8A_get()),
  /**
   * packed RGB 16:16:16, 48bpp, 16B, 16G, 16R, the 2-byte value for each R/G/B component is stored as big-endian
   */
    PIX_FMT_BGR48BE(VideoJNI.PixelFormat_PIX_FMT_BGR48BE_get()),
  /**
   * packed RGB 16:16:16, 48bpp, 16B, 16G, 16R, the 2-byte value for each R/G/B component is stored as little-endian
   */
    PIX_FMT_BGR48LE(VideoJNI.PixelFormat_PIX_FMT_BGR48LE_get()),
  /**
   * The following 12 formats have the disadvantage of needing 1 format for each bit depth.<br>
   * Notice that each 9/10 bits sample is stored in 16 bits with extra padding.<br>
   * If you want to support multiple bit depths, then using AV_PIX_FMT_YUV420P16* with the bpp stored separately is better.<br>
   * planar YUV 4:2:0, 13.5bpp, (1 Cr &amp; Cb sample per 2x2 Y samples), big-endian
   */
    PIX_FMT_YUV420P9BE(VideoJNI.PixelFormat_PIX_FMT_YUV420P9BE_get()),
  /**
   * planar YUV 4:2:0, 13.5bpp, (1 Cr &amp; Cb sample per 2x2 Y samples), little-endian
   */
    PIX_FMT_YUV420P9LE(VideoJNI.PixelFormat_PIX_FMT_YUV420P9LE_get()),
  /**
   * planar YUV 4:2:0, 15bpp, (1 Cr &amp; Cb sample per 2x2 Y samples), big-endian
   */
    PIX_FMT_YUV420P10BE(VideoJNI.PixelFormat_PIX_FMT_YUV420P10BE_get()),
  /**
   * planar YUV 4:2:0, 15bpp, (1 Cr &amp; Cb sample per 2x2 Y samples), little-endian
   */
    PIX_FMT_YUV420P10LE(VideoJNI.PixelFormat_PIX_FMT_YUV420P10LE_get()),
  /**
   * planar YUV 4:2:2, 20bpp, (1 Cr &amp; Cb sample per 2x1 Y samples), big-endian
   */
    PIX_FMT_YUV422P10BE(VideoJNI.PixelFormat_PIX_FMT_YUV422P10BE_get()),
  /**
   * planar YUV 4:2:2, 20bpp, (1 Cr &amp; Cb sample per 2x1 Y samples), little-endian
   */
    PIX_FMT_YUV422P10LE(VideoJNI.PixelFormat_PIX_FMT_YUV422P10LE_get()),
  /**
   * planar YUV 4:4:4, 27bpp, (1 Cr &amp; Cb sample per 1x1 Y samples), big-endian
   */
    PIX_FMT_YUV444P9BE(VideoJNI.PixelFormat_PIX_FMT_YUV444P9BE_get()),
  /**
   * planar YUV 4:4:4, 27bpp, (1 Cr &amp; Cb sample per 1x1 Y samples), little-endian
   */
    PIX_FMT_YUV444P9LE(VideoJNI.PixelFormat_PIX_FMT_YUV444P9LE_get()),
  /**
   * planar YUV 4:4:4, 30bpp, (1 Cr &amp; Cb sample per 1x1 Y samples), big-endian
   */
    PIX_FMT_YUV444P10BE(VideoJNI.PixelFormat_PIX_FMT_YUV444P10BE_get()),
  /**
   * planar YUV 4:4:4, 30bpp, (1 Cr &amp; Cb sample per 1x1 Y samples), little-endian
   */
    PIX_FMT_YUV444P10LE(VideoJNI.PixelFormat_PIX_FMT_YUV444P10LE_get()),
  /**
   * planar YUV 4:2:2, 18bpp, (1 Cr &amp; Cb sample per 2x1 Y samples), big-endian
   */
    PIX_FMT_YUV422P9BE(VideoJNI.PixelFormat_PIX_FMT_YUV422P9BE_get()),
  /**
   * planar YUV 4:2:2, 18bpp, (1 Cr &amp; Cb sample per 2x1 Y samples), little-endian
   */
    PIX_FMT_YUV422P9LE(VideoJNI.PixelFormat_PIX_FMT_YUV422P9LE_get()),
  /**
   * planar GBR 4:4:4 24bpp
   */
    PIX_FMT_GBRP(VideoJNI.PixelFormat_PIX_FMT_GBRP_get()),
    PIX_FMT_GBR24P(VideoJNI.PixelFormat_PIX_FMT_GBR24P_get()),
  /**
   * planar GBR 4:4:4 27bpp, big-endian
   */
    PIX_FMT_GBRP9BE(VideoJNI.PixelFormat_PIX_FMT_GBRP9BE_get()),
  /**
   * planar GBR 4:4:4 27bpp, little-endian
   */
    PIX_FMT_GBRP9LE(VideoJNI.PixelFormat_PIX_FMT_GBRP9LE_get()),
  /**
   * planar GBR 4:4:4 30bpp, big-endian
   */
    PIX_FMT_GBRP10BE(VideoJNI.PixelFormat_PIX_FMT_GBRP10BE_get()),
  /**
   * planar GBR 4:4:4 30bpp, little-endian
   */
    PIX_FMT_GBRP10LE(VideoJNI.PixelFormat_PIX_FMT_GBRP10LE_get()),
  /**
   * planar GBR 4:4:4 48bpp, big-endian
   */
    PIX_FMT_GBRP16BE(VideoJNI.PixelFormat_PIX_FMT_GBRP16BE_get()),
  /**
   * planar GBR 4:4:4 48bpp, little-endian
   */
    PIX_FMT_GBRP16LE(VideoJNI.PixelFormat_PIX_FMT_GBRP16LE_get()),
  /**
   * planar YUV 4:2:2 24bpp, (1 Cr &amp; Cb sample per 2x1 Y &amp; A samples)
   */
    PIX_FMT_YUVA422P(VideoJNI.PixelFormat_PIX_FMT_YUVA422P_get()),
  /**
   * planar YUV 4:4:4 32bpp, (1 Cr &amp; Cb sample per 1x1 Y &amp; A samples)
   */
    PIX_FMT_YUVA444P(VideoJNI.PixelFormat_PIX_FMT_YUVA444P_get()),
  /**
   * planar YUV 4:2:0 22.5bpp, (1 Cr &amp; Cb sample per 2x2 Y &amp; A samples), big-endian
   */
    PIX_FMT_YUVA420P9BE(VideoJNI.PixelFormat_PIX_FMT_YUVA420P9BE_get()),
  /**
   * planar YUV 4:2:0 22.5bpp, (1 Cr &amp; Cb sample per 2x2 Y &amp; A samples), little-endian
   */
    PIX_FMT_YUVA420P9LE(VideoJNI.PixelFormat_PIX_FMT_YUVA420P9LE_get()),
  /**
   * planar YUV 4:2:2 27bpp, (1 Cr &amp; Cb sample per 2x1 Y &amp; A samples), big-endian
   */
    PIX_FMT_YUVA422P9BE(VideoJNI.PixelFormat_PIX_FMT_YUVA422P9BE_get()),
  /**
   * planar YUV 4:2:2 27bpp, (1 Cr &amp; Cb sample per 2x1 Y &amp; A samples), little-endian
   */
    PIX_FMT_YUVA422P9LE(VideoJNI.PixelFormat_PIX_FMT_YUVA422P9LE_get()),
  /**
   * planar YUV 4:4:4 36bpp, (1 Cr &amp; Cb sample per 1x1 Y &amp; A samples), big-endian
   */
    PIX_FMT_YUVA444P9BE(VideoJNI.PixelFormat_PIX_FMT_YUVA444P9BE_get()),
  /**
   * planar YUV 4:4:4 36bpp, (1 Cr &amp; Cb sample per 1x1 Y &amp; A samples), little-endian
   */
    PIX_FMT_YUVA444P9LE(VideoJNI.PixelFormat_PIX_FMT_YUVA444P9LE_get()),
  /**
   * planar YUV 4:2:0 25bpp, (1 Cr &amp; Cb sample per 2x2 Y &amp; A samples, big-endian)
   */
    PIX_FMT_YUVA420P10BE(VideoJNI.PixelFormat_PIX_FMT_YUVA420P10BE_get()),
  /**
   * planar YUV 4:2:0 25bpp, (1 Cr &amp; Cb sample per 2x2 Y &amp; A samples, little-endian)
   */
    PIX_FMT_YUVA420P10LE(VideoJNI.PixelFormat_PIX_FMT_YUVA420P10LE_get()),
  /**
   * planar YUV 4:2:2 30bpp, (1 Cr &amp; Cb sample per 2x1 Y &amp; A samples, big-endian)
   */
    PIX_FMT_YUVA422P10BE(VideoJNI.PixelFormat_PIX_FMT_YUVA422P10BE_get()),
  /**
   * planar YUV 4:2:2 30bpp, (1 Cr &amp; Cb sample per 2x1 Y &amp; A samples, little-endian)
   */
    PIX_FMT_YUVA422P10LE(VideoJNI.PixelFormat_PIX_FMT_YUVA422P10LE_get()),
  /**
   * planar YUV 4:4:4 40bpp, (1 Cr &amp; Cb sample per 1x1 Y &amp; A samples, big-endian)
   */
    PIX_FMT_YUVA444P10BE(VideoJNI.PixelFormat_PIX_FMT_YUVA444P10BE_get()),
  /**
   * planar YUV 4:4:4 40bpp, (1 Cr &amp; Cb sample per 1x1 Y &amp; A samples, little-endian)
   */
    PIX_FMT_YUVA444P10LE(VideoJNI.PixelFormat_PIX_FMT_YUVA444P10LE_get()),
  /**
   * planar YUV 4:2:0 40bpp, (1 Cr &amp; Cb sample per 2x2 Y &amp; A samples, big-endian)
   */
    PIX_FMT_YUVA420P16BE(VideoJNI.PixelFormat_PIX_FMT_YUVA420P16BE_get()),
  /**
   * planar YUV 4:2:0 40bpp, (1 Cr &amp; Cb sample per 2x2 Y &amp; A samples, little-endian)
   */
    PIX_FMT_YUVA420P16LE(VideoJNI.PixelFormat_PIX_FMT_YUVA420P16LE_get()),
  /**
   * planar YUV 4:2:2 48bpp, (1 Cr &amp; Cb sample per 2x1 Y &amp; A samples, big-endian)
   */
    PIX_FMT_YUVA422P16BE(VideoJNI.PixelFormat_PIX_FMT_YUVA422P16BE_get()),
  /**
   * planar YUV 4:2:2 48bpp, (1 Cr &amp; Cb sample per 2x1 Y &amp; A samples, little-endian)
   */
    PIX_FMT_YUVA422P16LE(VideoJNI.PixelFormat_PIX_FMT_YUVA422P16LE_get()),
  /**
   * planar YUV 4:4:4 64bpp, (1 Cr &amp; Cb sample per 1x1 Y &amp; A samples, big-endian)
   */
    PIX_FMT_YUVA444P16BE(VideoJNI.PixelFormat_PIX_FMT_YUVA444P16BE_get()),
  /**
   * planar YUV 4:4:4 64bpp, (1 Cr &amp; Cb sample per 1x1 Y &amp; A samples, little-endian)
   */
    PIX_FMT_YUVA444P16LE(VideoJNI.PixelFormat_PIX_FMT_YUVA444P16LE_get()),
  /**
   * HW acceleration through VDPAU, Picture.data[3] contains a VdpVideoSurface
   */
    PIX_FMT_VDPAU(VideoJNI.PixelFormat_PIX_FMT_VDPAU_get()),
  /**
   * packed XYZ 4:4:4, 36 bpp, (msb) 12X, 12Y, 12Z (lsb), the 2-byte value for each X/Y/Z is stored as little-endian, the 4 lower bits are set to 0
   */
    PIX_FMT_XYZ12LE(VideoJNI.PixelFormat_PIX_FMT_XYZ12LE_get()),
  /**
   * packed XYZ 4:4:4, 36 bpp, (msb) 12X, 12Y, 12Z (lsb), the 2-byte value for each X/Y/Z is stored as big-endian, the 4 lower bits are set to 0
   */
    PIX_FMT_XYZ12BE(VideoJNI.PixelFormat_PIX_FMT_XYZ12BE_get()),
  /**
   * interleaved chroma YUV 4:2:2, 16bpp, (1 Cr &amp; Cb sample per 2x1 Y samples)
   */
    PIX_FMT_NV16(VideoJNI.PixelFormat_PIX_FMT_NV16_get()),
  /**
   * interleaved chroma YUV 4:2:2, 20bpp, (1 Cr &amp; Cb sample per 2x1 Y samples), little-endian
   */
    PIX_FMT_NV20LE(VideoJNI.PixelFormat_PIX_FMT_NV20LE_get()),
  /**
   * interleaved chroma YUV 4:2:2, 20bpp, (1 Cr &amp; Cb sample per 2x1 Y samples), big-endian
   */
    PIX_FMT_NV20BE(VideoJNI.PixelFormat_PIX_FMT_NV20BE_get()),
  /**
   * packed RGBA 16:16:16:16, 64bpp, 16R, 16G, 16B, 16A, the 2-byte value for each R/G/B/A component is stored as big-endian
   */
    PIX_FMT_RGBA64BE(VideoJNI.PixelFormat_PIX_FMT_RGBA64BE_get()),
  /**
   * packed RGBA 16:16:16:16, 64bpp, 16R, 16G, 16B, 16A, the 2-byte value for each R/G/B/A component is stored as little-endian
   */
    PIX_FMT_RGBA64LE(VideoJNI.PixelFormat_PIX_FMT_RGBA64LE_get()),
  /**
   * packed RGBA 16:16:16:16, 64bpp, 16B, 16G, 16R, 16A, the 2-byte value for each R/G/B/A component is stored as big-endian
   */
    PIX_FMT_BGRA64BE(VideoJNI.PixelFormat_PIX_FMT_BGRA64BE_get()),
  /**
   * packed RGBA 16:16:16:16, 64bpp, 16B, 16G, 16R, 16A, the 2-byte value for each R/G/B/A component is stored as little-endian
   */
    PIX_FMT_BGRA64LE(VideoJNI.PixelFormat_PIX_FMT_BGRA64LE_get()),
  /**
   * packed YUV 4:2:2, 16bpp, Y0 Cr Y1 Cb
   */
    PIX_FMT_YVYU422(VideoJNI.PixelFormat_PIX_FMT_YVYU422_get()),
  /**
   * 16 bits gray, 16 bits alpha (big-endian)
   */
    PIX_FMT_YA16BE(VideoJNI.PixelFormat_PIX_FMT_YA16BE_get()),
  /**
   * 16 bits gray, 16 bits alpha (little-endian)
   */
    PIX_FMT_YA16LE(VideoJNI.PixelFormat_PIX_FMT_YA16LE_get()),
  /**
   * planar GBRA 4:4:4:4 32bpp
   */
    PIX_FMT_GBRAP(VideoJNI.PixelFormat_PIX_FMT_GBRAP_get()),
  /**
   * planar GBRA 4:4:4:4 64bpp, big-endian
   */
    PIX_FMT_GBRAP16BE(VideoJNI.PixelFormat_PIX_FMT_GBRAP16BE_get()),
  /**
   * planar GBRA 4:4:4:4 64bpp, little-endian
   */
    PIX_FMT_GBRAP16LE(VideoJNI.PixelFormat_PIX_FMT_GBRAP16LE_get()),
  /**
   *  HW acceleration through QSV, data[3] contains a pointer to the<br>
   *  mfxFrameSurface1 structure.
   */
    PIX_FMT_QSV(VideoJNI.PixelFormat_PIX_FMT_QSV_get()),
  /**
   * HW acceleration though MMAL, data[3] contains a pointer to the<br>
   * MMAL_BUFFER_HEADER_T structure.
   */
    PIX_FMT_MMAL(VideoJNI.PixelFormat_PIX_FMT_MMAL_get()),
  /**
   * HW decoding through Direct3D11 via old API, Picture.data[3] contains a ID3D11VideoDecoderOutputView pointer
   */
    PIX_FMT_D3D11VA_VLD(VideoJNI.PixelFormat_PIX_FMT_D3D11VA_VLD_get()),
  /**
   * HW acceleration through CUDA. data[i] contain CUdeviceptr pointers<br>
   * exactly as for system memory frames.
   */
    PIX_FMT_CUDA(VideoJNI.PixelFormat_PIX_FMT_CUDA_get()),
  /**
   * packed RGB 8:8:8, 32bpp, XRGBXRGB...   X=unused/undefined
   */
    PIX_FMT_0RGB(VideoJNI.PixelFormat_PIX_FMT_0RGB_get()),
  /**
   * packed RGB 8:8:8, 32bpp, RGBXRGBX...   X=unused/undefined
   */
    PIX_FMT_RGB0(VideoJNI.PixelFormat_PIX_FMT_RGB0_get()),
  /**
   * packed BGR 8:8:8, 32bpp, XBGRXBGR...   X=unused/undefined
   */
    PIX_FMT_0BGR(VideoJNI.PixelFormat_PIX_FMT_0BGR_get()),
  /**
   * packed BGR 8:8:8, 32bpp, BGRXBGRX...   X=unused/undefined
   */
    PIX_FMT_BGR0(VideoJNI.PixelFormat_PIX_FMT_BGR0_get()),
  /**
   * planar YUV 4:2:0,18bpp, (1 Cr &amp; Cb sample per 2x2 Y samples), big-endian
   */
    PIX_FMT_YUV420P12BE(VideoJNI.PixelFormat_PIX_FMT_YUV420P12BE_get()),
  /**
   * planar YUV 4:2:0,18bpp, (1 Cr &amp; Cb sample per 2x2 Y samples), little-endian
   */
    PIX_FMT_YUV420P12LE(VideoJNI.PixelFormat_PIX_FMT_YUV420P12LE_get()),
  /**
   * planar YUV 4:2:0,21bpp, (1 Cr &amp; Cb sample per 2x2 Y samples), big-endian
   */
    PIX_FMT_YUV420P14BE(VideoJNI.PixelFormat_PIX_FMT_YUV420P14BE_get()),
  /**
   * planar YUV 4:2:0,21bpp, (1 Cr &amp; Cb sample per 2x2 Y samples), little-endian
   */
    PIX_FMT_YUV420P14LE(VideoJNI.PixelFormat_PIX_FMT_YUV420P14LE_get()),
  /**
   * planar YUV 4:2:2,24bpp, (1 Cr &amp; Cb sample per 2x1 Y samples), big-endian
   */
    PIX_FMT_YUV422P12BE(VideoJNI.PixelFormat_PIX_FMT_YUV422P12BE_get()),
  /**
   * planar YUV 4:2:2,24bpp, (1 Cr &amp; Cb sample per 2x1 Y samples), little-endian
   */
    PIX_FMT_YUV422P12LE(VideoJNI.PixelFormat_PIX_FMT_YUV422P12LE_get()),
  /**
   * planar YUV 4:2:2,28bpp, (1 Cr &amp; Cb sample per 2x1 Y samples), big-endian
   */
    PIX_FMT_YUV422P14BE(VideoJNI.PixelFormat_PIX_FMT_YUV422P14BE_get()),
  /**
   * planar YUV 4:2:2,28bpp, (1 Cr &amp; Cb sample per 2x1 Y samples), little-endian
   */
    PIX_FMT_YUV422P14LE(VideoJNI.PixelFormat_PIX_FMT_YUV422P14LE_get()),
  /**
   * planar YUV 4:4:4,36bpp, (1 Cr &amp; Cb sample per 1x1 Y samples), big-endian
   */
    PIX_FMT_YUV444P12BE(VideoJNI.PixelFormat_PIX_FMT_YUV444P12BE_get()),
  /**
   * planar YUV 4:4:4,36bpp, (1 Cr &amp; Cb sample per 1x1 Y samples), little-endian
   */
    PIX_FMT_YUV444P12LE(VideoJNI.PixelFormat_PIX_FMT_YUV444P12LE_get()),
  /**
   * planar YUV 4:4:4,42bpp, (1 Cr &amp; Cb sample per 1x1 Y samples), big-endian
   */
    PIX_FMT_YUV444P14BE(VideoJNI.PixelFormat_PIX_FMT_YUV444P14BE_get()),
  /**
   * planar YUV 4:4:4,42bpp, (1 Cr &amp; Cb sample per 1x1 Y samples), little-endian
   */
    PIX_FMT_YUV444P14LE(VideoJNI.PixelFormat_PIX_FMT_YUV444P14LE_get()),
  /**
   * planar GBR 4:4:4 36bpp, big-endian
   */
    PIX_FMT_GBRP12BE(VideoJNI.PixelFormat_PIX_FMT_GBRP12BE_get()),
  /**
   * planar GBR 4:4:4 36bpp, little-endian
   */
    PIX_FMT_GBRP12LE(VideoJNI.PixelFormat_PIX_FMT_GBRP12LE_get()),
  /**
   * planar GBR 4:4:4 42bpp, big-endian
   */
    PIX_FMT_GBRP14BE(VideoJNI.PixelFormat_PIX_FMT_GBRP14BE_get()),
  /**
   * planar GBR 4:4:4 42bpp, little-endian
   */
    PIX_FMT_GBRP14LE(VideoJNI.PixelFormat_PIX_FMT_GBRP14LE_get()),
  /**
   * planar YUV 4:1:1, 12bpp, (1 Cr &amp; Cb sample per 4x1 Y samples) full scale (JPEG), deprecated in favor of AV_PIX_FMT_YUV411P and setting color_range
   */
    PIX_FMT_YUVJ411P(VideoJNI.PixelFormat_PIX_FMT_YUVJ411P_get()),
  /**
   * bayer, BGBG..(odd line), GRGR..(even line), 8-bit samples 
   */
    PIX_FMT_BAYER_BGGR8(VideoJNI.PixelFormat_PIX_FMT_BAYER_BGGR8_get()),
  /**
   * bayer, RGRG..(odd line), GBGB..(even line), 8-bit samples 
   */
    PIX_FMT_BAYER_RGGB8(VideoJNI.PixelFormat_PIX_FMT_BAYER_RGGB8_get()),
  /**
   * bayer, GBGB..(odd line), RGRG..(even line), 8-bit samples 
   */
    PIX_FMT_BAYER_GBRG8(VideoJNI.PixelFormat_PIX_FMT_BAYER_GBRG8_get()),
  /**
   * bayer, GRGR..(odd line), BGBG..(even line), 8-bit samples 
   */
    PIX_FMT_BAYER_GRBG8(VideoJNI.PixelFormat_PIX_FMT_BAYER_GRBG8_get()),
  /**
   * bayer, BGBG..(odd line), GRGR..(even line), 16-bit samples, little-endian 
   */
    PIX_FMT_BAYER_BGGR16LE(VideoJNI.PixelFormat_PIX_FMT_BAYER_BGGR16LE_get()),
  /**
   * bayer, BGBG..(odd line), GRGR..(even line), 16-bit samples, big-endian 
   */
    PIX_FMT_BAYER_BGGR16BE(VideoJNI.PixelFormat_PIX_FMT_BAYER_BGGR16BE_get()),
  /**
   * bayer, RGRG..(odd line), GBGB..(even line), 16-bit samples, little-endian 
   */
    PIX_FMT_BAYER_RGGB16LE(VideoJNI.PixelFormat_PIX_FMT_BAYER_RGGB16LE_get()),
  /**
   * bayer, RGRG..(odd line), GBGB..(even line), 16-bit samples, big-endian 
   */
    PIX_FMT_BAYER_RGGB16BE(VideoJNI.PixelFormat_PIX_FMT_BAYER_RGGB16BE_get()),
  /**
   * bayer, GBGB..(odd line), RGRG..(even line), 16-bit samples, little-endian 
   */
    PIX_FMT_BAYER_GBRG16LE(VideoJNI.PixelFormat_PIX_FMT_BAYER_GBRG16LE_get()),
  /**
   * bayer, GBGB..(odd line), RGRG..(even line), 16-bit samples, big-endian 
   */
    PIX_FMT_BAYER_GBRG16BE(VideoJNI.PixelFormat_PIX_FMT_BAYER_GBRG16BE_get()),
  /**
   * bayer, GRGR..(odd line), BGBG..(even line), 16-bit samples, little-endian 
   */
    PIX_FMT_BAYER_GRBG16LE(VideoJNI.PixelFormat_PIX_FMT_BAYER_GRBG16LE_get()),
  /**
   * bayer, GRGR..(odd line), BGBG..(even line), 16-bit samples, big-endian 
   */
    PIX_FMT_BAYER_GRBG16BE(VideoJNI.PixelFormat_PIX_FMT_BAYER_GRBG16BE_get()),
  /**
   * XVideo Motion Acceleration via common packet passing
   */
    PIX_FMT_XVMC(VideoJNI.PixelFormat_PIX_FMT_XVMC_get()),
  /**
   * planar YUV 4:4:0,20bpp, (1 Cr &amp; Cb sample per 1x2 Y samples), little-endian
   */
    PIX_FMT_YUV440P10LE(VideoJNI.PixelFormat_PIX_FMT_YUV440P10LE_get()),
  /**
   * planar YUV 4:4:0,20bpp, (1 Cr &amp; Cb sample per 1x2 Y samples), big-endian
   */
    PIX_FMT_YUV440P10BE(VideoJNI.PixelFormat_PIX_FMT_YUV440P10BE_get()),
  /**
   * planar YUV 4:4:0,24bpp, (1 Cr &amp; Cb sample per 1x2 Y samples), little-endian
   */
    PIX_FMT_YUV440P12LE(VideoJNI.PixelFormat_PIX_FMT_YUV440P12LE_get()),
  /**
   * planar YUV 4:4:0,24bpp, (1 Cr &amp; Cb sample per 1x2 Y samples), big-endian
   */
    PIX_FMT_YUV440P12BE(VideoJNI.PixelFormat_PIX_FMT_YUV440P12BE_get()),
  /**
   * packed AYUV 4:4:4,64bpp (1 Cr &amp; Cb sample per 1x1 Y &amp; A samples), little-endian
   */
    PIX_FMT_AYUV64LE(VideoJNI.PixelFormat_PIX_FMT_AYUV64LE_get()),
  /**
   * packed AYUV 4:4:4,64bpp (1 Cr &amp; Cb sample per 1x1 Y &amp; A samples), big-endian
   */
    PIX_FMT_AYUV64BE(VideoJNI.PixelFormat_PIX_FMT_AYUV64BE_get()),
  /**
   * hardware decoding through Videotoolbox
   */
    PIX_FMT_VIDEOTOOLBOX(VideoJNI.PixelFormat_PIX_FMT_VIDEOTOOLBOX_get()),
  /**
   * like NV12, with 10bpp per component, data in the high bits, zeros in the low bits, little-endian
   */
    PIX_FMT_P010LE(VideoJNI.PixelFormat_PIX_FMT_P010LE_get()),
  /**
   * like NV12, with 10bpp per component, data in the high bits, zeros in the low bits, big-endian
   */
    PIX_FMT_P010BE(VideoJNI.PixelFormat_PIX_FMT_P010BE_get()),
  /**
   * planar GBR 4:4:4:4 48bpp, big-endian
   */
    PIX_FMT_GBRAP12BE(VideoJNI.PixelFormat_PIX_FMT_GBRAP12BE_get()),
  /**
   * planar GBR 4:4:4:4 48bpp, little-endian
   */
    PIX_FMT_GBRAP12LE(VideoJNI.PixelFormat_PIX_FMT_GBRAP12LE_get()),
  /**
   * planar GBR 4:4:4:4 40bpp, big-endian
   */
    PIX_FMT_GBRAP10BE(VideoJNI.PixelFormat_PIX_FMT_GBRAP10BE_get()),
  /**
   * planar GBR 4:4:4:4 40bpp, little-endian
   */
    PIX_FMT_GBRAP10LE(VideoJNI.PixelFormat_PIX_FMT_GBRAP10LE_get()),
  /**
   * hardware decoding through MediaCodec
   */
    PIX_FMT_MEDIACODEC(VideoJNI.PixelFormat_PIX_FMT_MEDIACODEC_get()),
  /**
   * Y        , 12bpp, big-endian
   */
    PIX_FMT_GRAY12BE(VideoJNI.PixelFormat_PIX_FMT_GRAY12BE_get()),
  /**
   * Y        , 12bpp, little-endian
   */
    PIX_FMT_GRAY12LE(VideoJNI.PixelFormat_PIX_FMT_GRAY12LE_get()),
  /**
   * Y        , 10bpp, big-endian
   */
    PIX_FMT_GRAY10BE(VideoJNI.PixelFormat_PIX_FMT_GRAY10BE_get()),
  /**
   * Y        , 10bpp, little-endian
   */
    PIX_FMT_GRAY10LE(VideoJNI.PixelFormat_PIX_FMT_GRAY10LE_get()),
  /**
   * like NV12, with 16bpp per component, little-endian
   */
    PIX_FMT_P016LE(VideoJNI.PixelFormat_PIX_FMT_P016LE_get()),
  /**
   * like NV12, with 16bpp per component, big-endian
   */
    PIX_FMT_P016BE(VideoJNI.PixelFormat_PIX_FMT_P016BE_get()),
  /**
   * Hardware surfaces for Direct3D11.<br>
   * <br>
   * This is preferred over the legacy AV_PIX_FMT_D3D11VA_VLD. The new D3D11<br>
   * hwaccel API and filtering support AV_PIX_FMT_D3D11 only.<br>
   * <br>
   * data[0] contains a ID3D11Texture2D pointer, and data[1] contains the<br>
   * texture array index of the frame as intptr_t if the ID3D11Texture2D is<br>
   * an array texture (or always 0 if it's a normal texture).
   */
    PIX_FMT_D3D11(VideoJNI.PixelFormat_PIX_FMT_D3D11_get()),
  /**
   * Y        , 9bpp, big-endian
   */
    PIX_FMT_GRAY9BE(VideoJNI.PixelFormat_PIX_FMT_GRAY9BE_get()),
  /**
   * Y        , 9bpp, little-endian
   */
    PIX_FMT_GRAY9LE(VideoJNI.PixelFormat_PIX_FMT_GRAY9LE_get()),
  /**
   * IEEE-754 single precision planar GBR 4:4:4,     96bpp, big-endian
   */
    PIX_FMT_GBRPF32BE(VideoJNI.PixelFormat_PIX_FMT_GBRPF32BE_get()),
  /**
   * IEEE-754 single precision planar GBR 4:4:4,     96bpp, little-endian
   */
    PIX_FMT_GBRPF32LE(VideoJNI.PixelFormat_PIX_FMT_GBRPF32LE_get()),
  /**
   * IEEE-754 single precision planar GBRA 4:4:4:4, 128bpp, big-endian
   */
    PIX_FMT_GBRAPF32BE(VideoJNI.PixelFormat_PIX_FMT_GBRAPF32BE_get()),
  /**
   * IEEE-754 single precision planar GBRA 4:4:4:4, 128bpp, little-endian
   */
    PIX_FMT_GBRAPF32LE(VideoJNI.PixelFormat_PIX_FMT_GBRAPF32LE_get()),
  /**
   * DRM-managed buffers exposed through PRIME buffer sharing.<br>
   * <br>
   * data[0] points to an AVDRMFrameDescriptor.
   */
    PIX_FMT_DRM_PRIME(VideoJNI.PixelFormat_PIX_FMT_DRM_PRIME_get()),
  /**
   * Hardware surfaces for OpenCL.<br>
   * <br>
   * data[i] contain 2D image objects (typed in C as cl_mem, used<br>
   * in OpenCL as image2d_t) for each plane of the surface.
   */
    PIX_FMT_OPENCL(VideoJNI.PixelFormat_PIX_FMT_OPENCL_get()),
  /**
   * Y        , 14bpp, big-endian
   */
    PIX_FMT_GRAY14BE(VideoJNI.PixelFormat_PIX_FMT_GRAY14BE_get()),
  /**
   * Y        , 14bpp, little-endian
   */
    PIX_FMT_GRAY14LE(VideoJNI.PixelFormat_PIX_FMT_GRAY14LE_get()),
  /**
   * IEEE-754 single precision Y, 32bpp, big-endian
   */
    PIX_FMT_GRAYF32BE(VideoJNI.PixelFormat_PIX_FMT_GRAYF32BE_get()),
  /**
   * IEEE-754 single precision Y, 32bpp, little-endian
   */
    PIX_FMT_GRAYF32LE(VideoJNI.PixelFormat_PIX_FMT_GRAYF32LE_get()),
  ;

    public final int swigValue() {
      return swigValue;
    }

    public static Type swigToEnum(int swigValue) {
      Type[] swigValues = Type.class.getEnumConstants();
      if (swigValue < swigValues.length && swigValue >= 0 && swigValues[swigValue].swigValue == swigValue)
        return swigValues[swigValue];
      for (Type swigEnum : swigValues)
        if (swigEnum.swigValue == swigValue)
          return swigEnum;
      throw new IllegalArgumentException("No enum " + Type.class + " with value " + swigValue);
    }

    @SuppressWarnings("unused")
    private Type() {
      this.swigValue = SwigNext.next++;
    }

    @SuppressWarnings("unused")
    private Type(int swigValue) {
      this.swigValue = swigValue;
      SwigNext.next = swigValue+1;
    }

    @SuppressWarnings("unused")
    private Type(Type swigEnum) {
      this.swigValue = swigEnum.swigValue;
      SwigNext.next = this.swigValue+1;
    }

    private final int swigValue;

    private static class SwigNext {
      private static int next = 0;
    }
  }

}
